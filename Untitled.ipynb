{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T08:48:44.458000Z",
     "start_time": "2018-12-19T08:48:44.393000Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data,label = datasets.make_classification(n_samples= 10000, n_features=20, n_informative= 6 ,\n",
    "             n_classes=2, n_clusters_per_class=10,random_state=0)\n",
    "dfdata = pd.DataFrame(data,columns = [u'f'+str(i) for i in range(data.shape[1])])\n",
    "dfdata['label'] = label\n",
    "dftrain,dftest = train_test_split(dfdata)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(dftrain.drop('label',axis = 1), dftrain['label'])\n",
    "dvalid = xgb.DMatrix(dftest.drop('label',axis = 1), dftest['label'])\n",
    "dtest = xgb.DMatrix(dftest.drop('label',axis = 1), dftest['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T08:03:14.138000Z",
     "start_time": "2018-12-19T08:03:14.108000Z"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化参数\n",
    "params_dict = dict()\n",
    "\n",
    "# 以下为待调整参数\n",
    "# booster参数\n",
    "params_dict['learning_rate'] = 0.1       # 学习率，初始值为 0.1，通常越小越好。\n",
    "params_dict['n_estimators'] = 60         # 加法模型树的数量，初始值为50。\n",
    "\n",
    "# tree参数\n",
    "params_dict['max_depth'] = 3              # 树的深度，通常取值在[3,10]之间，初始值常取[3,6]之间\n",
    "params_dict['min_child_weight']= 30       # 最小叶子节点样本权重和，越大模型越保守。\n",
    "params_dict['gamma']= 0                   # 节点分裂所需的最小损失函数下降值，越大模型越保守。\n",
    "params_dict['subsample']= 0.8             # 横向采样，样本采样比例，通常取值在 [0.5，1]之间 \n",
    "params_dict['colsample_bytree'] = 1.0     # 纵向采样，特征采样比例，通常取值在 [0.5，1]之间 \n",
    "\n",
    "# regulazation参数 \n",
    "# Omega(f) = gamma*T + reg_alpha* sum(abs(wj)) + reg_lambda* sum(wj**2)  \n",
    "\n",
    "params_dict['reg_alpha'] = 0              #L1 正则化项的权重系数，越大模型越保守，通常取值在[0,1]之间。\n",
    "params_dict['reg_lambda'] = 1             #L2 正则化项的权重系数，越大模型越保守，通常取值在[1,100]之间。\n",
    "\n",
    "# 以下参数通常不需要调整\n",
    "params_dict['objective'] = 'binary:logistic'\n",
    "params_dict['tree_method'] = 'hist'       # 构建树的策略,可以是auto, exact, approx, hist\n",
    "params_dict['eval_metric'] =  'auc'\n",
    "params_dict['silent'] = 1\n",
    "params_dict['nthread'] = 2\n",
    "params_dict['scale_pos_weight'] = 1        #不平衡样本时设定为正值可以使算法更快收敛。\n",
    "params_dict['seed'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T08:03:17.098000Z",
     "start_time": "2018-12-19T08:03:17.076000Z"
    }
   },
   "outputs": [],
   "source": [
    "def auc(labels, preds):\n",
    "    \"\"\"\n",
    "    auc值的大小可以理解为: 随机抽一个正样本和一个负样本，正样本预测值比负样本大的概率\n",
    "    最简单粗暴的方法\n",
    "　　先排序，然后统计有多少正负样本对满足：正样本预测值>负样本预测值, 再除以总的正负样本对个数\n",
    "    复杂度 O(NlogN), N为样本数\n",
    "    \"\"\"\n",
    "    n_pos = sum(labels)\n",
    "    n_neg = len(labels) - n_pos\n",
    "    total_pair = n_pos * n_neg\n",
    " \n",
    "    labels_preds = zip(labels, preds)\n",
    "    labels_preds = sorted(labels_preds, key=lambda x: x[1])\n",
    "    accumulated_neg = 0\n",
    "    satisfied_pair = 0\n",
    "    for i in range(len(labels_preds)):\n",
    "        if labels_preds[i][0] == 1:\n",
    "            satisfied_pair += accumulated_neg\n",
    "        else:\n",
    "            accumulated_neg += 1\n",
    "    return satisfied_pair / float(total_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T04:36:56.374000Z",
     "start_time": "2018-12-19T04:36:56.354000Z"
    }
   },
   "outputs": [],
   "source": [
    "dtrain.slice(np.arange(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T04:33:39.948000Z",
     "start_time": "2018-12-19T04:33:39.924000Z"
    }
   },
   "outputs": [],
   "source": [
    "bst.save_model('msg.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T04:34:17.488000Z",
     "start_time": "2018-12-19T04:34:17.470000Z"
    }
   },
   "outputs": [],
   "source": [
    "booster = xgb.Booster(model_file='msg.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:38:43.547000Z",
     "start_time": "2018-12-19T03:38:43.525000Z"
    }
   },
   "outputs": [],
   "source": [
    "bst.get_score(importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T06:24:51.299000Z",
     "start_time": "2018-12-19T06:24:51.290000Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T08:03:22.248000Z",
     "start_time": "2018-12-19T08:03:22.217000Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys,os,json,datetime\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "\n",
    "# 定义ks评分指标,供xgboost.train函数的feval调用\n",
    "def ks_feval(preds,xgbtrain):\n",
    "    label = xgbtrain.get_label()\n",
    "    assert len(preds) == len(label)\n",
    "    df = pd.DataFrame(data = np.array([preds,label]).T,columns = ['preds','label'])\n",
    "    df_0,df_1 = df[df['label']<0.5],df[df['label']>=0.5]\n",
    "    ks,ks_pvalue = stats.ks_2samp(df_0['preds'].values,df_1['preds'].values)\n",
    "    return 'ks',ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T08:03:26.109000Z",
     "start_time": "2018-12-19T08:03:26.041000Z"
    }
   },
   "outputs": [],
   "source": [
    "def auc(labels, preds):\n",
    "    \"\"\"\n",
    "    auc值的大小可以理解为: 随机抽一个正样本和一个负样本，正样本预测值比负样本大的概率\n",
    "　　先排序，然后统计有多少正负样本对满足：正样本预测值>负样本预测值, 再除以总的正负样本对个数\n",
    "    \"\"\"\n",
    "    n_pos = sum(labels)\n",
    "    n_neg = len(labels) - n_pos\n",
    "    total_pair = n_pos * n_neg\n",
    " \n",
    "    labels_preds = zip(labels, preds)\n",
    "    labels_preds = sorted(labels_preds, key=lambda x: x[1])\n",
    "    accumulated_neg = 0\n",
    "    satisfied_pair = 0\n",
    "    for i in range(len(labels_preds)):\n",
    "        if labels_preds[i][0] == 1:\n",
    "            satisfied_pair += accumulated_neg\n",
    "        else:\n",
    "            accumulated_neg += 1\n",
    "    return satisfied_pair / float(total_pair)\n",
    "    \n",
    "def stratified_kfold(data,label,nfolds = 5):\n",
    "\n",
    "    label = np.array(label)\n",
    "    assert len(data) == len(label), 'the length of data and label not match!'\n",
    "    assert set(label) == {0,1}, 'label can only be 0 or 1!'\n",
    "    \n",
    "    index = np.arange(len(label))\n",
    "    index_0 = index[label<0.5].copy()\n",
    "    index_1 = index[label>0.5].copy()\n",
    "    np.random.shuffle(index_0)\n",
    "    np.random.shuffle(index_1)\n",
    "    \n",
    "    split_points_0 = (len(index_0) * np.arange(1,nfolds))//nfolds\n",
    "    split_points_1 = (len(index_1) * np.arange(1,nfolds))//nfolds\n",
    "    split_index_0_list = np.split(index_0,split_points_0)\n",
    "    split_index_1_list = np.split(index_1,split_points_1)\n",
    "    split_index_list = [np.concatenate((x,y)) for x,y in \n",
    "                     zip(split_index_0_list,split_index_1_list)]\n",
    "    \n",
    "    result = [(np.setdiff1d(index,x),x) for x in split_index_list]\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T08:26:20.609000Z",
     "start_time": "2018-12-19T08:26:20.578000Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练xgb模型\n",
    "def train_xgb(params_dict,dtrain,dvalid = None,dtest = None,verbose_eval = 10):\n",
    "    \n",
    "    result = {}\n",
    "    watchlist = [x for x in [(dtrain, 'train'),(dvalid,'valid'),(dtest,'test')] if x[0] is not None]\n",
    "    datasets = [x[1] for x in watchlist]\n",
    "    \n",
    "    bst = xgb.train(params = params_dict, dtrain = dtrain, \n",
    "                    num_boost_round = params_dict.get('n_estimators',100), \n",
    "                    feval = ks_feval,verbose_eval= verbose_eval,\n",
    "                    evals = watchlist,\n",
    "                    evals_result = result)\n",
    "    dfresult = pd.DataFrame({(dataset+'_'+feval): result[dataset][feval] \n",
    "               for dataset in datasets for feval in ('auc','ks')})\n",
    "    \n",
    "    return bst,dfresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T08:30:59.252000Z",
     "start_time": "2018-12-19T08:30:58.741000Z"
    }
   },
   "outputs": [],
   "source": [
    "bst,_ = train_xgb(params_dict,dtrain,None,None,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T08:38:30.233000Z",
     "start_time": "2018-12-19T08:38:30.189000Z"
    }
   },
   "outputs": [],
   "source": [
    "dfimportance = dfimportance.sort('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T08:39:43.714000Z",
     "start_time": "2018-12-19T08:39:43.685000Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_scores = bst.get_score()\n",
    "dfimportance = pd.DataFrame({'feature':feature_scores.keys(),'importance':feature_scores.values()})\n",
    "try:\n",
    "    dfimportance = dfimportance.sort_values('importance',ascending=False)\n",
    "except AttributeError as err:\n",
    "    dfimportance = dfimportance.sort('importance',ascending = False)\n",
    "    \n",
    "dfimportance.index = range(len(dfimportance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T08:58:21.439000Z",
     "start_time": "2018-12-19T08:58:20.697000Z"
    }
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "#!/usr/bin/python2.7\n",
    "\n",
    "##################################################\n",
    "#update_dt:2018-12-19\n",
    "#author：liangyun\n",
    "#usage: run xgboost model\n",
    "##################################################\n",
    "from __future__ import print_function\n",
    "import datetime,sys,os\n",
    "import ks,outliers,dropfeature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# 定义ks评分指标,供xgboost.train函数的feval调用\n",
    "def ks_feval(preds,xgbtrain):\n",
    "    label = xgbtrain.get_label()\n",
    "    assert len(preds) == len(label)\n",
    "    df = pd.DataFrame(data = np.array([preds,label]).T,columns = ['preds','label'])\n",
    "    df_0,df_1 = df[df['label']<0.5],df[df['label']>=0.5]\n",
    "    ks,ks_pvalue = stats.ks_2samp(df_0['preds'].values,df_1['preds'].values)\n",
    "    return 'ks',ks\n",
    "\n",
    "def auc(labels, preds):\n",
    "    \"\"\"\n",
    "    auc值的大小可以理解为: 随机抽一个正样本和一个负样本，正样本预测值比负样本大的概率\n",
    "　　先排序，然后统计有多少正负样本对满足：正样本预测值>负样本预测值, 再除以总的正负样本对个数\n",
    "    \"\"\"\n",
    "    n_pos = sum(labels)\n",
    "    n_neg = len(labels) - n_pos\n",
    "    total_pair = n_pos * n_neg\n",
    " \n",
    "    labels_preds = zip(labels, preds)\n",
    "    labels_preds = sorted(labels_preds, key=lambda x: x[1])\n",
    "    accumulated_neg = 0\n",
    "    satisfied_pair = 0\n",
    "    for i in range(len(labels_preds)):\n",
    "        if labels_preds[i][0] == 1:\n",
    "            satisfied_pair += accumulated_neg\n",
    "        else:\n",
    "            accumulated_neg += 1\n",
    "    return satisfied_pair / float(total_pair)\n",
    "    \n",
    "def stratified_kfold(data,label,nfolds = 5):\n",
    "\n",
    "    label = np.array(label)\n",
    "    assert len(data) == len(label), 'the length of data and label not match!'\n",
    "    assert set(label) == {0,1}, 'label can only be 0 or 1!'\n",
    "    \n",
    "    index = np.arange(len(label))\n",
    "    index_0 = index[label<0.5].copy()\n",
    "    index_1 = index[label>0.5].copy()\n",
    "    np.random.shuffle(index_0)\n",
    "    np.random.shuffle(index_1)\n",
    "    \n",
    "    split_points_0 = (len(index_0) * np.arange(1,nfolds))//nfolds\n",
    "    split_points_1 = (len(index_1) * np.arange(1,nfolds))//nfolds\n",
    "    split_index_0_list = np.split(index_0,split_points_0)\n",
    "    split_index_1_list = np.split(index_1,split_points_1)\n",
    "    split_index_list = [np.concatenate((x,y)) for x,y in \n",
    "                     zip(split_index_0_list,split_index_1_list)]\n",
    "    \n",
    "    result = [(np.setdiff1d(index,x),x) for x in split_index_list]\n",
    "    return result\n",
    "\n",
    "\n",
    "# 训练xgb模型\n",
    "def train_xgb(params_dict,dtrain,dvalid = None,dtest = None,verbose_eval = 10):\n",
    "    \n",
    "    result = {}\n",
    "    watchlist = [x for x in [(dtrain, 'train'),(dvalid,'valid'),(dtest,'test')] if x[0] is not None]\n",
    "    datasets = [x[1] for x in watchlist]\n",
    "    \n",
    "    bst = xgb.train(params = params_dict, dtrain = dtrain, \n",
    "                    num_boost_round = params_dict.get('n_estimators',100), \n",
    "                    feval = ks_feval,verbose_eval= verbose_eval,\n",
    "                    evals = watchlist,\n",
    "                    evals_result = result)\n",
    "    dfresult = pd.DataFrame({(dataset+'_'+feval): result[dataset][feval] \n",
    "               for dataset in datasets for feval in ('auc','ks')})\n",
    "    \n",
    "    return bst,dfresult\n",
    "\n",
    "class RunXgboost(object):\n",
    "    \"\"\"\n",
    "    Examples\n",
    "    ---------\n",
    "    # 准备训练数据\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    data,label = datasets.make_classification(n_samples= 10000, n_features=20,n_classes=2, random_state=0)\n",
    "    dfdata = pd.DataFrame(data,columns = ['feature'+str(i) for i in range(data.shape[1])])\n",
    "    dfdata['label'] = label\n",
    "    dftrain,dftest = train_test_split(dfdata)\n",
    "    dftrain,dftest = dftrain.copy(),dftest.copy()\n",
    "    dftrain.index,dftest.index  = range(len(dftrain)),range(len(dftest))\n",
    "    dftrain.loc[0,['feature0','feature1','feature2']] = np.nan #构造若干缺失值\n",
    "    \n",
    "    \n",
    "    # 配置xgboost模型参数\n",
    "    params_dict = dict()\n",
    "\n",
    "    # 以下为待调整参数\n",
    "    # booster参数\n",
    "    params_dict['learning_rate'] = 0.1       # 学习率，初始值为 0.1，通常越小越好。\n",
    "    params_dict['n_estimators'] = 60         # 加法模型树的数量，初始值为50。\n",
    "    \n",
    "    # tree参数\n",
    "    params_dict['max_depth'] = 3              # 树的深度，通常取值在[3,10]之间，初始值常取[3,6]之间\n",
    "    params_dict['min_child_weight']= 30       # 最小叶子节点样本权重和，越大模型越保守。\n",
    "    params_dict['gamma']= 0                   # 节点分裂所需的最小损失函数下降值，越大模型越保守。\n",
    "    params_dict['subsample']= 0.8             # 横向采样，样本采样比例，通常取值在 [0.5，1]之间 \n",
    "    params_dict['colsample_bytree'] = 1.0     # 纵向采样，特征采样比例，通常取值在 [0.5，1]之间 \n",
    "\n",
    "    # regulazation参数 \n",
    "    # Omega(f) = gamma*T + reg_alpha* sum(abs(wj)) + reg_lambda* sum(wj**2)  \n",
    "\n",
    "    params_dict['reg_alpha'] = 0              #L1 正则化项的权重系数，越大模型越保守，通常取值在[0,1]之间。\n",
    "    params_dict['reg_lambda'] = 1             #L2 正则化项的权重系数，越大模型越保守，通常取值在[1,100]之间。\n",
    "\n",
    "    # 以下参数通常不需要调整\n",
    "    params_dict['objective'] = 'binary:logistic'\n",
    "    params_dict['tree_method'] = 'hist'       # 构建树的策略,可以是auto, exact, approx, hist\n",
    "    params_dict['eval_metric'] =  'auc'\n",
    "    params_dict['silent'] = 1\n",
    "    params_dict['nthread'] = 2\n",
    "    params_dict['scale_pos_weight'] = 1        #不平衡样本时设定为正值可以使算法更快收敛。\n",
    "    params_dict['seed'] = 0\n",
    "    \n",
    "    # 训练xgboost模型\n",
    "    from tianjikit.runxgboost import RunXgboost\n",
    "    model = RunXgboost(dftrain = dftrain,dftest = dftest, coverage_th=0, ks_th=0,\n",
    "            outliers_th=None, selected_features=None)\n",
    "    bst = model.train(cv=5, model_idx=5,params_dict = params_dict) \n",
    "    model.test(bst,dftest)\n",
    "    dfimportance = model.dfimportance\n",
    "    bst.save_model('bst.model')\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dftrain,dftest = '',coverage_th = 0, ks_th = 0, outliers_th = None,\n",
    "                  selected_features = None):\n",
    "        \n",
    "        # 校验是否有label列\n",
    "        assert 'label' in dftrain.columns, 'illegal input,there should be a  \"label\" column in dftrain!'\n",
    "        \n",
    "        # 校验label列的合法性\n",
    "        assert set(dftrain['label']) == {0,1},'illegal label values,label can only be 0 or 1!'\n",
    "        \n",
    "        self.dftrain = dftrain\n",
    "        self.dftest = dftest\n",
    "        \n",
    "        # 记录预处理参数信息\n",
    "        self.coverage_th = coverage_th\n",
    "        self.ks_th = ks_th\n",
    "        self.outliers_th = outliers_th\n",
    "        self.selected_features = selected_features\n",
    "        \n",
    "        X_train,y_train,X_test,y_test = self.preprocess_data(self.dftrain,self.dftest)\n",
    "        \n",
    "       \n",
    "        # 预处理后的训练和验证集\n",
    "        self.X_train,self.y_train = X_train,y_train\n",
    "        self.X_test,self.y_test  = X_test,y_test\n",
    "        \n",
    "        \n",
    "        # 特征重要性\n",
    "        self.dfimportance = None \n",
    "        \n",
    "        # 报告信息\n",
    "        self.report_info = ''\n",
    "        \n",
    "        \n",
    "    def preprocess_data(self,dftrain,dftest):\n",
    "        \n",
    "        # 输出预处理提示信息\n",
    "        nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print('\\n================================================================================ %s\\n'%nowtime)\n",
    "        print('start data preprocessing ...\\n')\n",
    "        print('train set size:  {}'.format(len(dftrain)))\n",
    "        print('test set size:  {}'.format(len(dftest)))\n",
    "        print('coverage threshold:  {}'.format(self.coverage_th))\n",
    "        print('outlier threshold:  {}'.format(self.outliers_th))\n",
    "        print('ks threshold:  {}'.format(self.ks_th))\n",
    "\n",
    "       \n",
    "        # 去掉['phone','id','idcard','id_card','loan_dt','name','id_map']等非特征列\n",
    "        for  col in {'phone','id','unique_id','uniq_id','idcard','id-card','id_card','name','loan_dt','idmap','id_map','id-map'}:\n",
    "            if col in dftrain.columns:\n",
    "                dftrain = dftrain.drop(col,axis = 1)\n",
    "                if len(dftest):dftest = dftest.drop(col,axis = 1)\n",
    "                    \n",
    "        # 校验是否存在非数值列 \n",
    "        try:\n",
    "            assert not np.dtype('O') in dftrain.dtypes.values\n",
    "        except:\n",
    "            object_cols = dftrain.columns[dftrain.dtypes == np.object].tolist()\n",
    "            print('removed feature columns not numerical: %s'%(','.join(map(str,object_cols))),file = sys.stderr)\n",
    "            dftrain = dftrain.drop(object_cols,axis = 1)\n",
    "            if len(dftest):dftest = dftest.drop(object_cols,axis = 1)\n",
    "        \n",
    "        # 如果selected_features 不为空，则进行特征筛选\n",
    "        if self.selected_features:\n",
    "            remained_cols = [col for col in dftrain.columns if col in self.selected_features + ['label']]\n",
    "            dftrain = dftrain[remained_cols]\n",
    "            if len(dftest):dftest = dftest[remained_cols]\n",
    "                    \n",
    "        # 分割feature和label\n",
    "        X_train = dftrain.drop(['label'],axis = 1)\n",
    "        y_train = dftrain[['label']]\n",
    "        X_test = dftest.drop(['label'],axis = 1) if len(dftest) else ''\n",
    "        y_test = dftest[['label']] if len(dftest) else ''\n",
    "        \n",
    "        print('original feature number:  {}'.format(X_train.shape[1]))\n",
    "        \n",
    "        # drop_outliers()\n",
    "        if self.outliers_th:\n",
    "            for col in X_train.columns:\n",
    "                X_train[col] = outliers.drop_outliers(X_train[col].values, X_train[col].values, alpha = self.outliers_th) \n",
    "                if len(dftest): X_test[col] = outliers.drop_outliers(X_train[col].values,X_test[col].values, alpha = self.outliers_th)  \n",
    "                    \n",
    "        # drop_feature()\n",
    "        X_train, X_test = dropfeature.drop_feature(X_train,y_train,X_test,coverage_threshold = self.coverage_th, \n",
    "                          ks_threshold = self.ks_th) \n",
    "        \n",
    "        print('feature number remain after dropfeature:  {}'.format(X_train.shape[1]))\n",
    "        \n",
    "        # 重置index  \n",
    "        X_train.index = range(len(X_train))\n",
    "        y_train.index = range(len(X_train))\n",
    "        X_test.index = range(len(X_test)) \n",
    "        y_test.index = range(len(X_test))\n",
    "        \n",
    "        return(X_train,y_train,X_test,y_test)\n",
    "    \n",
    "    \n",
    "    def train(self,cv = 5,model_idx = 5,\n",
    "              params_dict = {'n_estimators': 50,'objective':'binary:logistic','eval_metric':'auc','silent':1},\n",
    "              verbose_eval = 20):\n",
    "        \n",
    "        info = \"start train xgboost model ...\"\n",
    "        print(info)\n",
    "        self.report_info = self.report_info + info + '\\n'\n",
    "        \n",
    "        dtrain = xgb.DMatrix(self.X_train, self.y_train['label'])\n",
    "        \n",
    "        if cv:\n",
    "            \n",
    "            k,ks_mean_train,auc_mean_train,ks_mean_validate,auc_mean_validate = 0,0,0,0,0\n",
    "\n",
    "            models = {}\n",
    "\n",
    "            for train_index,validate_index in stratified_kfold(self.X_train,np.ravel(self.y_train),nfolds = cv):\n",
    "\n",
    "                k = k + 1\n",
    "                nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                info = '\\n{}: k = {}'.format(nowtime,k)\n",
    "                print(info)\n",
    "                self.report_info = self.report_info + info + '\\n'\n",
    "                \n",
    "                dtrain_k,dvalid_k = dtrain.slice(train_index),dtrain.slice(validate_index)\n",
    "                bst,_ = train_xgb(params_dict,dtrain_k,dvalid_k,None,verbose_eval)\n",
    "                predict_train_k = bst.predict(dtrain_k)\n",
    "                predict_validate_k = bst.predict(dvalid_k)\n",
    "\n",
    "                dfks_train = ks.ks_analysis(predict_train_k,dtrain_k.get_label())\n",
    "                dfks_validate = ks.ks_analysis(predict_validate_k,dvalid_k.get_label())\n",
    "\n",
    "                ks_train,ks_validate = max(dfks_train['ks_value']),max(dfks_validate['ks_value'])\n",
    "                \n",
    "                auc_train = auc(dtrain_k.get_label(),predict_train_k)\n",
    "                auc_validate = auc(dvalid_k.get_label(), predict_validate_k)\n",
    "        \n",
    "                ks_mean_train = ks_mean_train + ks_train\n",
    "                auc_mean_train = auc_mean_train + auc_train\n",
    "                ks_mean_validate = ks_mean_validate + ks_validate\n",
    "                auc_mean_validate = auc_mean_validate + auc_validate\n",
    "\n",
    "                info = '\\ntrain: ks = {} \\t auc = {} '.format(ks_train,auc_train)\n",
    "                prettyks = ks.print_ks(predict_train_k,dtrain_k.get_label())\n",
    "                info = info + '\\n' + str(prettyks) + '\\n'\n",
    "                info = info + '\\nvalidate: ks = {} \\t auc = {}'.format(ks_validate,auc_validate) + '\\n'\n",
    "                prettyks = ks.print_ks(predict_validate_k,dvalid_k.get_label())\n",
    "                info = info + str(prettyks) + '\\n'\n",
    "                print(info)\n",
    "                self.report_info = self.report_info + info\n",
    "                \n",
    "                models[k] = bst\n",
    "\n",
    "            ks_mean_train = ks_mean_train/float(k)\n",
    "            auc_mean_train = auc_mean_train/float(k)\n",
    "            ks_mean_validate = ks_mean_validate/float(k)\n",
    "            auc_mean_validate = auc_mean_validate/float(k)\n",
    "            \n",
    "            nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            info = '\\n================================================================================ %s\\n'%nowtime\n",
    "            info = info + 'train : ks mean {:.5f} ; auc mean {:.5f}'.format(ks_mean_train, auc_mean_train) + '\\n'\n",
    "            info = info + 'validate : ks mean {:.5f} ; auc mean {:.5f}'.format(ks_mean_validate, auc_mean_validate) + '\\n'\n",
    "            print(info)\n",
    "            self.report_info = self.report_info + info\n",
    "\n",
    "            bst = models[model_idx]\n",
    "            \n",
    "        # 处理 cv = 0 或 cv = None时无需交叉验证逻辑\n",
    "        else:\n",
    "            \n",
    "            nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            info = '\\n================================================================================ %s\\n'%nowtime\n",
    "            print(info)\n",
    "            self.report_info = self.report_info + info\n",
    "            \n",
    "            bst,_ = train_xgb(params_dict,dtrain,None,None,verbose_eval)\n",
    "            predict_train = bst.predict(dtrain)\n",
    "            dfks_train = ks.ks_analysis(predict_train,self.y_train.values)\n",
    "            ks_train = max(dfks_train['ks_value'])   \n",
    "            auc_train = auc(dtrain.get_label(),predict_train)\n",
    "            \n",
    "            info = '\\ntrain: ks = {} \\t auc = {} '.format(ks_train,auc_train) + '\\n'\n",
    "            prettyks = ks.print_ks(predict_train,self.y_train.values)\n",
    "            info = info + str(prettyks) + '\\n'\n",
    "            print(info)\n",
    "            self.report_info = self.report_info + info\n",
    "            \n",
    "        # 计算特征重要性\n",
    "        feature_scores = bst.get_score()\n",
    "        dfimportance = pd.DataFrame({'feature':feature_scores.keys(),'importance':feature_scores.values()})\n",
    "        try:\n",
    "            dfimportance = dfimportance.sort_values('importance',ascending=False)\n",
    "        except AttributeError as err:\n",
    "            dfimportance = dfimportance.sort('importance',ascending = False)\n",
    "\n",
    "        dfimportance.index = range(len(dfimportance))\n",
    "        \n",
    "        self.dfimportance = dfimportance\n",
    "        \n",
    "        return(bst)\n",
    "        \n",
    "    def test(self,bst,dftest = pd.DataFrame()):\n",
    "        \n",
    "        info = \"\\nstart test xgboost model ... \"\n",
    "        print(info)\n",
    "        self.report_info = self.report_info + info + '\\n'\n",
    "        \n",
    "        # 若传入新的dftest，则需要再次做数据预处理\n",
    "        if len(dftest)>0:\n",
    "            \n",
    "            print('preprocessing test data...\\n')\n",
    "            \n",
    "            # 禁止数据预处理期间打印输出\n",
    "            stdout = sys.stdout\n",
    "            sys.stdout = open(os.devnull, 'w')\n",
    "            \n",
    "            X_train,y_train,X_test,y_test = self.preprocess_data(self.dftrain,dftest)\n",
    "            \n",
    "            # 恢复打印输出\n",
    "            sys.stdout = stdout\n",
    "            \n",
    "            # 预处理后的训练和测试集\n",
    "            self.X_train,self.y_train = X_train,y_train\n",
    "            self.X_test,self.y_test  = X_test,y_test\n",
    "            \n",
    "        dtest = xgb.DMatrix(self.X_test, self.y_test['label'])\n",
    "        y_test_hat = bst.predict(dtest)\n",
    "        dfks_test = ks.ks_analysis(y_test_hat,np.ravel(self.y_test))\n",
    "        ks_test = max(dfks_test['ks_value'])\n",
    "        auc_test = auc(np.ravel(self.y_test),y_test_hat)\n",
    "        \n",
    "        info = 'test: ks = {} \\t auc = {} '.format(ks_test,auc_test) + '\\n'\n",
    "        prettyks = ks.print_ks(y_test_hat,np.ravel(self.y_test))\n",
    "        info = info + str(prettyks) + '\\n'\n",
    "        print(info)\n",
    "        self.report_info = self.report_info + info + '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:52:51.449000Z",
     "start_time": "2018-12-19T09:52:50.280000Z"
    }
   },
   "outputs": [],
   "source": [
    "# 准备训练数据\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data,label = datasets.make_classification(n_samples= 10000, n_features=20,n_classes=2, random_state=0)\n",
    "dfdata = pd.DataFrame(data,columns = ['feature'+str(i) for i in range(data.shape[1])])\n",
    "dfdata['label'] = label\n",
    "dftrain,dftest = train_test_split(dfdata)\n",
    "dftrain,dftest = dftrain.copy(),dftest.copy()\n",
    "dftrain.index,dftest.index  = range(len(dftrain)),range(len(dftest))\n",
    "dftrain.loc[0,['feature0','feature1','feature2']] = np.nan #构造若干缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:52:56.016000Z",
     "start_time": "2018-12-19T09:52:55.986000Z"
    }
   },
   "outputs": [],
   "source": [
    "# 配置xgboost模型参数\n",
    "params_dict = dict()\n",
    "\n",
    "# 以下为待调整参数\n",
    "# booster参数\n",
    "params_dict['learning_rate'] = 0.1       # 学习率，初始值为 0.1，通常越小越好。\n",
    "params_dict['n_estimators'] = 60         # 加法模型树的数量，初始值为50。\n",
    "\n",
    "# tree参数\n",
    "params_dict['max_depth'] = 3              # 树的深度，通常取值在[3,10]之间，初始值常取[3,6]之间\n",
    "params_dict['min_child_weight']= 30       # 最小叶子节点样本权重和，越大模型越保守。\n",
    "params_dict['gamma']= 0                   # 节点分裂所需的最小损失函数下降值，越大模型越保守。\n",
    "params_dict['subsample']= 0.8             # 横向采样，样本采样比例，通常取值在 [0.5，1]之间 \n",
    "params_dict['colsample_bytree'] = 1.0     # 纵向采样，特征采样比例，通常取值在 [0.5，1]之间 \n",
    "\n",
    "# regulazation参数 \n",
    "# Omega(f) = gamma*T + reg_alpha* sum(abs(wj)) + reg_lambda* sum(wj**2)  \n",
    "\n",
    "params_dict['reg_alpha'] = 0              #L1 正则化项的权重系数，越大模型越保守，通常取值在[0,1]之间。\n",
    "params_dict['reg_lambda'] = 1             #L2 正则化项的权重系数，越大模型越保守，通常取值在[1,100]之间。\n",
    "\n",
    "# 以下参数通常不需要调整\n",
    "params_dict['objective'] = 'binary:logistic'\n",
    "params_dict['tree_method'] = 'hist'       # 构建树的策略,可以是auto, exact, approx, hist\n",
    "params_dict['eval_metric'] =  'auc'\n",
    "params_dict['silent'] = 1\n",
    "params_dict['nthread'] = 2\n",
    "params_dict['scale_pos_weight'] = 1        #不平衡样本时设定为正值可以使算法更快收敛。\n",
    "params_dict['seed'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T08:58:36.356000Z",
     "start_time": "2018-12-19T08:58:29.175000Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练xgboost模型\n",
    "\n",
    "model = RunXgboost(dftrain = dftrain,dftest = dftest, coverage_th=0, ks_th=0,\n",
    "        outliers_th=None, selected_features=None)\n",
    "bst = model.train(cv=5, model_idx=5) \n",
    "model.test(bst,dftest)\n",
    "dfimportance = model.dfimportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:06:42.748000Z",
     "start_time": "2018-12-19T09:06:42.739000Z"
    }
   },
   "outputs": [],
   "source": [
    "from tianjikit.runxgboost import RunXgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:06:53.231000Z",
     "start_time": "2018-12-19T09:06:53.222000Z"
    }
   },
   "outputs": [],
   "source": [
    "?RunXgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:14:59.579000Z",
     "start_time": "2018-12-19T09:14:59.569000Z"
    }
   },
   "outputs": [],
   "source": [
    "bst.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:09:40.354000Z",
     "start_time": "2018-12-19T09:09:40.202000Z"
    }
   },
   "outputs": [],
   "source": [
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:10:34.604000Z",
     "start_time": "2018-12-19T09:10:34.468000Z"
    }
   },
   "outputs": [],
   "source": [
    "model.X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:10:24.852000Z",
     "start_time": "2018-12-19T09:10:24.677000Z"
    }
   },
   "outputs": [],
   "source": [
    "model.X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:53:14.190000Z",
     "start_time": "2018-12-19T09:53:10.295000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================ 2018-12-19 17:53:10\n",
      "\n",
      "start data preprocessing ...\n",
      "\n",
      "train set size:  7500\n",
      "test set size:  2500\n",
      "coverage threshold:  0\n",
      "outlier threshold:  None\n",
      "ks threshold:  0\n",
      "original feature number:  30\n",
      "feature number remain after dropfeature:  30\n",
      "start train xgboost model ...\n",
      "\n",
      "2018-12-19 17:53:10: k = 1\n",
      "[17:53:10] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[0]\ttrain-auc:0.906302\tvalid-auc:0.903551\ttrain-ks:0.691673\tvalid-ks:0.689678\n",
      "[10]\ttrain-auc:0.957146\tvalid-auc:0.955447\ttrain-ks:0.772651\tvalid-ks:0.772659\n",
      "[20]\ttrain-auc:0.965713\tvalid-auc:0.961877\ttrain-ks:0.821941\tvalid-ks:0.811944\n",
      "[30]\ttrain-auc:0.967707\tvalid-auc:0.962963\ttrain-ks:0.825533\tvalid-ks:0.814355\n",
      "[40]\ttrain-auc:0.968601\tvalid-auc:0.964427\ttrain-ks:0.826731\tvalid-ks:0.819938\n",
      "[50]\ttrain-auc:0.969961\tvalid-auc:0.964497\ttrain-ks:0.829125\tvalid-ks:0.820761\n",
      "[59]\ttrain-auc:0.971284\tvalid-auc:0.964284\ttrain-ks:0.833524\tvalid-ks:0.823157\n",
      "\n",
      "train: ks = 0.8248 \t auc = 0.971283661622 \n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "|   |  feature_interval | order_num | order_ratio | overdue_num | overdue_ratio | ks_value |\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "| 0 | [0.01131,0.01723) |    500    |     0.1     |      2      |     0.004     | 0.19856  |\n",
      "| 1 | [0.01723,0.02237) |    500    |     0.1     |      2      |     0.004     | 0.39712  |\n",
      "| 2 |  [0.02237,0.0294) |    500    |     0.1     |      2      |     0.004     | 0.59568  |\n",
      "| 3 |  [0.0294,0.16713) |    500    |     0.1     |      21     |     0.042     | 0.77904  |\n",
      "| 4 | [0.16713,0.61774) |    500    |     0.1     |     193     |     0.386     |  0.8248  |\n",
      "| 5 | [0.61774,0.78418) |    500    |     0.1     |     370     |      0.74     | 0.72896  |\n",
      "| 6 | [0.78418,0.92906) |    500    |     0.1     |     454     |     0.908     | 0.56592  |\n",
      "| 7 | [0.92906,0.95679) |    500    |     0.1     |     471     |     0.942     | 0.38928  |\n",
      "| 8 | [0.95679,0.96808) |    500    |     0.1     |     488     |     0.976     | 0.19904  |\n",
      "| 9 | [0.96808,0.98226] |    500    |     0.1     |     499     |     0.998     |   0.0    |\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "\n",
      "validate: ks = 0.8008 \t auc = 0.964284137142\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "|   |  feature_interval | order_num | order_ratio | overdue_num | overdue_ratio | ks_value |\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "| 0 | [0.01225,0.01727) |    250    |     0.1     |      0      |      0.0      | 0.20016  |\n",
      "| 1 | [0.01727,0.02216) |    250    |     0.1     |      4      |     0.016     | 0.39392  |\n",
      "| 2 | [0.02216,0.03014) |    250    |     0.1     |      4      |     0.016     | 0.58769  |\n",
      "| 3 | [0.03014,0.16212) |    250    |     0.1     |      10     |      0.04     | 0.77184  |\n",
      "| 4 | [0.16212,0.61498) |    250    |     0.1     |     107     |     0.428     |  0.8008  |\n",
      "| 5 |  [0.61498,0.7704) |    250    |     0.1     |     172     |     0.688     | 0.72576  |\n",
      "| 6 |  [0.7704,0.93118) |    250    |     0.1     |     229     |     0.916     | 0.55952  |\n",
      "| 7 | [0.93118,0.95672) |    250    |     0.1     |     240     |      0.96     | 0.37568  |\n",
      "| 8 | [0.95672,0.96786) |    250    |     0.1     |     239     |     0.956     | 0.19344  |\n",
      "| 9 | [0.96786,0.98304] |    250    |     0.1     |     246     |     0.984     |   0.0    |\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "\n",
      "\n",
      "2018-12-19 17:53:11: k = 2\n",
      "[17:53:11] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[0]\ttrain-auc:0.915788\tvalid-auc:0.904398\ttrain-ks:0.698118\tvalid-ks:0.671327\n",
      "[10]\ttrain-auc:0.958851\tvalid-auc:0.955399\ttrain-ks:0.776859\tvalid-ks:0.776643\n",
      "[20]\ttrain-auc:0.966491\tvalid-auc:0.964147\ttrain-ks:0.818345\tvalid-ks:0.819145\n",
      "[30]\ttrain-auc:0.967947\tvalid-auc:0.964838\ttrain-ks:0.826759\tvalid-ks:0.816744\n",
      "[40]\ttrain-auc:0.968846\tvalid-auc:0.966222\ttrain-ks:0.827154\tvalid-ks:0.82472\n",
      "[50]\ttrain-auc:0.970584\tvalid-auc:0.965905\ttrain-ks:0.829921\tvalid-ks:0.823128\n",
      "[59]\ttrain-auc:0.971379\tvalid-auc:0.965475\ttrain-ks:0.832325\tvalid-ks:0.823115\n",
      "\n",
      "train: ks = 0.8304 \t auc = 0.971378861682 \n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "|   |  feature_interval | order_num | order_ratio | overdue_num | overdue_ratio | ks_value |\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "| 0 | [0.01266,0.01913) |    500    |     0.1     |      1      |     0.002     | 0.19936  |\n",
      "| 1 | [0.01913,0.02349) |    500    |     0.1     |      3      |     0.006     | 0.39712  |\n",
      "| 2 |  [0.02349,0.0295) |    500    |     0.1     |      6      |     0.012     | 0.59248  |\n",
      "| 3 |  [0.0295,0.19807) |    500    |     0.1     |      17     |     0.034     | 0.77904  |\n",
      "| 4 | [0.19807,0.61065) |    500    |     0.1     |     186     |     0.372     |  0.8304  |\n",
      "| 5 | [0.61065,0.79887) |    500    |     0.1     |     374     |     0.748     | 0.73136  |\n",
      "| 6 | [0.79887,0.93823) |    500    |     0.1     |     458     |     0.916     | 0.56512  |\n",
      "| 7 | [0.93823,0.95582) |    500    |     0.1     |     467     |     0.934     | 0.39168  |\n",
      "| 8 | [0.95582,0.96638) |    500    |     0.1     |     491     |     0.982     | 0.19904  |\n",
      "| 9 | [0.96638,0.98288] |    500    |     0.1     |     499     |     0.998     |   0.0    |\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "\n",
      "validate: ks = 0.8072 \t auc = 0.965475177904\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "|   |  feature_interval | order_num | order_ratio | overdue_num | overdue_ratio | ks_value |\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "| 0 | [0.01288,0.01921) |    250    |     0.1     |      1      |     0.004     | 0.19856  |\n",
      "| 1 | [0.01921,0.02376) |    250    |     0.1     |      0      |      0.0      | 0.39872  |\n",
      "| 2 | [0.02376,0.02918) |    250    |     0.1     |      3      |     0.012     | 0.59408  |\n",
      "| 3 |  [0.02918,0.2019) |    250    |     0.1     |      8      |     0.032     | 0.78144  |\n",
      "| 4 |  [0.2019,0.61796) |    250    |     0.1     |     109     |     0.436     |  0.8072  |\n",
      "| 5 | [0.61796,0.78144) |    250    |     0.1     |     183     |     0.732     | 0.71456  |\n",
      "| 6 | [0.78144,0.93822) |    250    |     0.1     |     224     |     0.896     | 0.55632  |\n",
      "| 7 | [0.93822,0.95527) |    250    |     0.1     |     235     |      0.94     | 0.38048  |\n",
      "| 8 | [0.95527,0.96611) |    250    |     0.1     |     243     |     0.972     | 0.19184  |\n",
      "| 9 | [0.96611,0.98229] |    250    |     0.1     |     245     |      0.98     |   0.0    |\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "\n",
      "\n",
      "2018-12-19 17:53:12: k = 3\n",
      "[17:53:12] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[0]\ttrain-auc:0.900844\tvalid-auc:0.893553\ttrain-ks:0.663717\tvalid-ks:0.659311\n",
      "[10]\ttrain-auc:0.955916\tvalid-auc:0.948802\ttrain-ks:0.773085\tvalid-ks:0.745486\n",
      "[20]\ttrain-auc:0.967246\tvalid-auc:0.960751\ttrain-ks:0.817509\tvalid-ks:0.809546\n",
      "[30]\ttrain-auc:0.969341\tvalid-auc:0.962044\ttrain-ks:0.827151\tvalid-ks:0.81434\n",
      "[40]\ttrain-auc:0.970435\tvalid-auc:0.96255\ttrain-ks:0.832765\tvalid-ks:0.815954\n",
      "[50]\ttrain-auc:0.971601\tvalid-auc:0.962156\ttrain-ks:0.836355\tvalid-ks:0.815151\n",
      "[59]\ttrain-auc:0.972768\tvalid-auc:0.962154\ttrain-ks:0.839565\tvalid-ks:0.815951\n",
      "\n",
      "train: ks = 0.8328 \t auc = 0.972768462572 \n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "|   |  feature_interval | order_num | order_ratio | overdue_num | overdue_ratio | ks_value |\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "| 0 | [0.01208,0.01581) |    500    |     0.1     |      0      |      0.0      | 0.20016  |\n",
      "| 1 | [0.01581,0.02182) |    500    |     0.1     |      4      |     0.008     | 0.39712  |\n",
      "| 2 | [0.02182,0.02902) |    500    |     0.1     |      4      |     0.008     | 0.59408  |\n",
      "| 3 | [0.02902,0.16204) |    500    |     0.1     |      17     |     0.034     | 0.78064  |\n",
      "| 4 | [0.16204,0.60268) |    500    |     0.1     |     185     |      0.37     |  0.8328  |\n",
      "| 5 | [0.60268,0.78085) |    500    |     0.1     |     376     |     0.752     | 0.73216  |\n",
      "| 6 | [0.78085,0.94154) |    500    |     0.1     |     454     |     0.908     | 0.56912  |\n",
      "| 7 |  [0.94154,0.9596) |    500    |     0.1     |     473     |     0.946     | 0.39088  |\n",
      "| 8 |  [0.9596,0.97089) |    500    |     0.1     |     489     |     0.978     | 0.19984  |\n",
      "| 9 |  [0.97089,0.9841] |    500    |     0.1     |     500     |      1.0      |   0.0    |\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "\n",
      "validate: ks = 0.7944 \t auc = 0.962154855779\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "|   |  feature_interval | order_num | order_ratio | overdue_num | overdue_ratio | ks_value |\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "| 0 | [0.01156,0.01551) |    250    |     0.1     |      3      |     0.012     | 0.19536  |\n",
      "| 1 | [0.01551,0.02165) |    250    |     0.1     |      1      |     0.004     | 0.39392  |\n",
      "| 2 | [0.02165,0.02922) |    250    |     0.1     |      2      |     0.008     | 0.59088  |\n",
      "| 3 |  [0.02922,0.196)  |    250    |     0.1     |      12     |     0.048     | 0.77184  |\n",
      "| 4 |   [0.196,0.6049)  |    250    |     0.1     |     111     |     0.444     |  0.7944  |\n",
      "| 5 |  [0.6049,0.80356) |    250    |     0.1     |     174     |     0.696     | 0.71616  |\n",
      "| 6 | [0.80356,0.94336) |    250    |     0.1     |     226     |     0.904     | 0.55472  |\n",
      "| 7 |  [0.94336,0.9593) |    250    |     0.1     |     236     |     0.944     | 0.37728  |\n",
      "| 8 |  [0.9593,0.97082) |    250    |     0.1     |     241     |     0.964     | 0.19184  |\n",
      "| 9 | [0.97082,0.98304] |    250    |     0.1     |     245     |      0.98     |   0.0    |\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "\n",
      "\n",
      "================================================================================ 2018-12-19 17:53:13\n",
      "train : ks mean 0.82933 ; auc mean 0.97181\n",
      "validate : ks mean 0.80080 ; auc mean 0.96397\n",
      "\n",
      "\n",
      "start test xgboost model ... \n",
      "preprocessing test data...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: ks = 0.83047 \t auc = 0.969318772418 \n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "|   |  feature_interval | order_num | order_ratio | overdue_num | overdue_ratio | ks_value |\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "| 0 | [0.01249,0.01705) |    250    |     0.1     |      1      |     0.004     | 0.20034  |\n",
      "| 1 | [0.01705,0.02171) |    250    |     0.1     |      2      |     0.008     | 0.39907  |\n",
      "| 2 | [0.02171,0.02896) |    250    |     0.1     |      4      |     0.016     | 0.59461  |\n",
      "| 3 | [0.02896,0.17406) |    250    |     0.1     |      12     |     0.048     | 0.77735  |\n",
      "| 4 | [0.17406,0.62693) |    250    |     0.1     |      93     |     0.372     | 0.83047  |\n",
      "| 5 | [0.62693,0.81612) |    250    |     0.1     |     192     |     0.768     | 0.72518  |\n",
      "| 6 | [0.81612,0.93627) |    250    |     0.1     |     233     |     0.932     | 0.55429  |\n",
      "| 7 | [0.93627,0.95758) |    250    |     0.1     |     239     |     0.956     | 0.37379  |\n",
      "| 8 | [0.95758,0.96883) |    250    |     0.1     |     237     |     0.948     |  0.1965  |\n",
      "| 9 | [0.96883,0.98277] |    250    |     0.1     |     249     |     0.996     |   0.0    |\n",
      "+---+-------------------+-----------+-------------+-------------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 准备训练数据\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data,label = datasets.make_classification(n_samples= 10000, n_features=30,n_classes=2, random_state=0)\n",
    "dfdata = pd.DataFrame(data,columns = ['feature'+str(i) for i in range(data.shape[1])])\n",
    "dfdata['label'] = label\n",
    "dftrain,dftest = train_test_split(dfdata)\n",
    "dftrain,dftest = dftrain.copy(),dftest.copy()\n",
    "dftrain.index,dftest.index  = range(len(dftrain)),range(len(dftest))\n",
    "dftrain.loc[0,['feature0','feature1','feature2']] = np.nan #构造若干缺失值\n",
    "\n",
    "\n",
    "# 配置xgboost模型参数\n",
    "params_dict = dict()\n",
    "\n",
    "# 以下为待调整参数\n",
    "# booster参数\n",
    "params_dict['learning_rate'] = 0.1       # 学习率，初始值为 0.1，通常越小越好。\n",
    "params_dict['n_estimators'] = 60         # 加法模型树的数量，初始值为50。\n",
    "\n",
    "# tree参数\n",
    "params_dict['max_depth'] = 3              # 树的深度，通常取值在[3,10]之间，初始值常取[3,6]之间\n",
    "params_dict['min_child_weight']= 30       # 最小叶子节点样本权重和，越大模型越保守。\n",
    "params_dict['gamma']= 0                   # 节点分裂所需的最小损失函数下降值，越大模型越保守。\n",
    "params_dict['subsample']= 0.8             # 横向采样，样本采样比例，通常取值在 [0.5，1]之间 \n",
    "params_dict['colsample_bytree'] = 1.0     # 纵向采样，特征采样比例，通常取值在 [0.5，1]之间 \n",
    "\n",
    "# regulazation参数 \n",
    "# Omega(f) = gamma*T + reg_alpha* sum(abs(wj)) + reg_lambda* sum(wj**2)  \n",
    "\n",
    "params_dict['reg_alpha'] = 0              #L1 正则化项的权重系数，越大模型越保守，通常取值在[0,1]之间。\n",
    "params_dict['reg_lambda'] = 1             #L2 正则化项的权重系数，越大模型越保守，通常取值在[1,100]之间。\n",
    "\n",
    "# 以下参数通常不需要调整\n",
    "params_dict['objective'] = 'binary:logistic'\n",
    "params_dict['tree_method'] = 'hist'       # 构建树的策略,可以是auto, exact, approx, hist\n",
    "params_dict['eval_metric'] =  'auc'\n",
    "params_dict['silent'] = 1\n",
    "params_dict['nthread'] = 2\n",
    "params_dict['scale_pos_weight'] = 1        #不平衡样本时设定为正值可以使算法更快收敛。\n",
    "params_dict['seed'] = 0\n",
    "\n",
    "# 训练xgboost模型\n",
    "from tianjikit.runxgboost import RunXgboost\n",
    "model = RunXgboost(dftrain = dftrain,dftest = dftest, coverage_th=0, ks_th=0,\n",
    "        outliers_th=None, selected_features=None)\n",
    "bst = model.train(cv=3, model_idx=1,params_dict = params_dict,verbose_eval = 10) \n",
    "model.test(bst,dftest)\n",
    "dfimportance = model.dfimportance\n",
    "#bst.save_model('bst.model')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:24:49.739000Z",
     "start_time": "2018-12-19T09:24:49.721000Z"
    }
   },
   "outputs": [],
   "source": [
    "bst.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:26:02.985000Z",
     "start_time": "2018-12-19T09:26:02.966000Z"
    }
   },
   "outputs": [],
   "source": [
    "model.dtrain.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T09:37:27.627000Z",
     "start_time": "2018-12-19T09:37:27.613000Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "data = pd.DataFrame(np.random.randn(5,3),columns = list('abc'))\n",
    "dtrain = xgb.DMatrix(data)\n",
    "dslice = dtrain.slice([0,1,2])\n",
    "print(dtrain.feature_names)\n",
    "print(dslice.feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
