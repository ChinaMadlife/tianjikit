{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:25:46.671000Z",
     "start_time": "2018-12-19T03:25:46.611000Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data,label = datasets.make_classification(n_samples= 10000, n_features=20, n_informative= 6 ,\n",
    "             n_classes=2, n_clusters_per_class=10,random_state=0)\n",
    "dfdata = pd.DataFrame(data,columns = [u'中国@_abc.f'+str(i) for i in range(data.shape[1])])\n",
    "dfdata['label'] = label\n",
    "dftrain,dftest = train_test_split(dfdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:24:36.808000Z",
     "start_time": "2018-12-19T03:24:36.777000Z"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化参数\n",
    "params_dict = dict()\n",
    "\n",
    "# 以下为待调整参数\n",
    "# booster参数\n",
    "params_dict['learning_rate'] = 0.1       # 学习率，初始值为 0.1，通常越小越好。\n",
    "params_dict['n_estimators'] = 60         # 加法模型树的数量，初始值为50。\n",
    "\n",
    "# tree参数\n",
    "params_dict['max_depth'] = 3              # 树的深度，通常取值在[3,10]之间，初始值常取[3,6]之间\n",
    "params_dict['min_child_weight']= 30       # 最小叶子节点样本权重和，越大模型越保守。\n",
    "params_dict['gamma']= 0                   # 节点分裂所需的最小损失函数下降值，越大模型越保守。\n",
    "params_dict['subsample']= 0.8             # 横向采样，样本采样比例，通常取值在 [0.5，1]之间 \n",
    "params_dict['colsample_bytree'] = 1.0     # 纵向采样，特征采样比例，通常取值在 [0.5，1]之间 \n",
    "\n",
    "# regulazation参数 \n",
    "# Omega(f) = gamma*T + reg_alpha* sum(abs(wj)) + reg_lambda* sum(wj**2)  \n",
    "\n",
    "params_dict['reg_alpha'] = 0              #L1 正则化项的权重系数，越大模型越保守，通常取值在[0,1]之间。\n",
    "params_dict['reg_lambda'] = 1             #L2 正则化项的权重系数，越大模型越保守，通常取值在[1,100]之间。\n",
    "\n",
    "# 以下参数通常不需要调整\n",
    "params_dict['objective'] = 'binary:logistic'\n",
    "params_dict['tree_method'] = 'hist'       # 构建树的策略,可以是auto, exact, approx, hist\n",
    "params_dict['eval_metric'] =  'auc'\n",
    "params_dict['silent'] = 1\n",
    "params_dict['nthread'] = 2\n",
    "params_dict['scale_pos_weight'] = 1        #不平衡样本时设定为正值可以使算法更快收敛。\n",
    "params_dict['seed'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:25:51.351000Z",
     "start_time": "2018-12-19T03:25:51.100000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:25:51] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(dftrain.drop('label',axis = 1), dftrain['label'])\n",
    "dvalid = xgb.DMatrix(dftest.drop('label',axis = 1), dftest['label'])\n",
    "dtest = xgb.DMatrix(dftest.drop('label',axis = 1), dftest['label'])\n",
    "\n",
    "bst = xgb.train(params = params_dict, dtrain = dtrain, \n",
    "                num_boost_round = params_dict['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T04:36:56.374000Z",
     "start_time": "2018-12-19T04:36:56.354000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100L"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain.slice(np.arange(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T04:33:39.948000Z",
     "start_time": "2018-12-19T04:33:39.924000Z"
    }
   },
   "outputs": [],
   "source": [
    "bst.save_model('msg.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T04:34:17.488000Z",
     "start_time": "2018-12-19T04:34:17.470000Z"
    }
   },
   "outputs": [],
   "source": [
    "booster = xgb.Booster(model_file='msg.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:38:43.547000Z",
     "start_time": "2018-12-19T03:38:43.525000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'\\u4e2d\\u56fd@_abc.f0': 9.08396149,\n",
       " u'\\u4e2d\\u56fd@_abc.f1': 25.15135812030303,\n",
       " u'\\u4e2d\\u56fd@_abc.f10': 41.938080745781235,\n",
       " u'\\u4e2d\\u56fd@_abc.f11': 6.25136948,\n",
       " u'\\u4e2d\\u56fd@_abc.f12': 2.82127762,\n",
       " u'\\u4e2d\\u56fd@_abc.f13': 17.62893230555555,\n",
       " u'\\u4e2d\\u56fd@_abc.f15': 20.852823002820507,\n",
       " u'\\u4e2d\\u56fd@_abc.f17': 6.437244416666666,\n",
       " u'\\u4e2d\\u56fd@_abc.f18': 6.425109385,\n",
       " u'\\u4e2d\\u56fd@_abc.f19': 62.315462803947376,\n",
       " u'\\u4e2d\\u56fd@_abc.f2': 34.80039481826923,\n",
       " u'\\u4e2d\\u56fd@_abc.f3': 7.6599724275000005,\n",
       " u'\\u4e2d\\u56fd@_abc.f4': 28.268154418571424,\n",
       " u'\\u4e2d\\u56fd@_abc.f6': 6.381576535000001,\n",
       " u'\\u4e2d\\u56fd@_abc.f7': 7.52671242,\n",
       " u'\\u4e2d\\u56fd@_abc.f8': 6.59309673,\n",
       " u'\\u4e2d\\u56fd@_abc.f9': 20.556989746562497}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.get_score(importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T04:42:22.971000Z",
     "start_time": "2018-12-19T04:42:22.955000Z"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T04:50:09.246000Z",
     "start_time": "2018-12-19T04:50:08.880000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:50:08] Tree method is selected to be 'hist', which uses a single updater grow_fast_histmaker.\n",
      "[0]\ttrain-auc:0.716575\ttest-auc:0.707066\n",
      "[10]\ttrain-auc:0.775136\ttest-auc:0.759548\n",
      "[20]\ttrain-auc:0.792214\ttest-auc:0.768672\n",
      "[30]\ttrain-auc:0.799127\ttest-auc:0.771439\n",
      "[40]\ttrain-auc:0.808913\ttest-auc:0.779022\n",
      "[50]\ttrain-auc:0.816417\ttest-auc:0.784047\n",
      "[59]\ttrain-auc:0.821445\ttest-auc:0.787254\n"
     ]
    }
   ],
   "source": [
    "result = {}\n",
    "bst = xgb.train(params = params_dict, dtrain = dtrain, \n",
    "                num_boost_round = params_dict['n_estimators'], \n",
    "                verbose_eval=10,\n",
    "                evals = [(dtrain, 'train'),(dtest, 'test')], \n",
    "                evals_result = result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练xgb模型\n",
    "def train_xgb(params_dict,dtrain):\n",
    "    \n",
    "    result = {}\n",
    "    bst = xgb.train(params = params_dict, dtrain = dtrain, \n",
    "                    num_boost_round = params_dict.get('n_estimators',100))\n",
    "    dfresult = pd.DataFrame({(dataset+'_'+feval): result[dataset][feval] \n",
    "               for dataset in ('train','valid','test') for feval in ('auc','ks')})\n",
    "    \n",
    "    return bst,dfresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "#!/usr/bin/python2.7\n",
    "\n",
    "##################################################\n",
    "#update_dt:2018-10-08\n",
    "#author：liangyun\n",
    "#usage: run model\n",
    "##################################################\n",
    "from __future__ import print_function\n",
    "import datetime,sys,os\n",
    "import ks,outliers,dropfeature,fillnan,scalefeature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "    \n",
    "    \n",
    "def stratified_kfold(data,label,nfolds = 5):\n",
    "\n",
    "    label = np.array(label)\n",
    "    assert len(data) == len(label), 'the length of data and label not match!'\n",
    "    assert set(label) == {0,1}, 'label can only be 0 or 1!'\n",
    "    \n",
    "    index = np.arange(len(label))\n",
    "    index_0 = index[label<0.5].copy()\n",
    "    index_1 = index[label>0.5].copy()\n",
    "    np.random.shuffle(index_0)\n",
    "    np.random.shuffle(index_1)\n",
    "    \n",
    "    split_points_0 = (len(index_0) * np.arange(1,nfolds))//nfolds\n",
    "    split_points_1 = (len(index_1) * np.arange(1,nfolds))//nfolds\n",
    "    split_index_0_list = np.split(index_0,split_points_0)\n",
    "    split_index_1_list = np.split(index_1,split_points_1)\n",
    "    split_index_list = [np.concatenate((x,y)) for x,y in \n",
    "                     zip(split_index_0_list,split_index_1_list)]\n",
    "    \n",
    "    result = [(np.setdiff1d(index,x),x) for x in split_index_list]\n",
    "    return result\n",
    "\n",
    "\n",
    "class RunModel(object):\n",
    "    \"\"\"\n",
    "    Examples\n",
    "    ---------\n",
    "    # 准备训练数据\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    data,label = datasets.make_classification(n_samples= 10000, n_features=20,n_classes=2, random_state=0)\n",
    "    dfdata = pd.DataFrame(data,columns = ['feature'+str(i) for i in range(data.shape[1])])\n",
    "    dfdata['label'] = label\n",
    "    dftrain,dftest = train_test_split(dfdata)\n",
    "    dftrain,dftest = dftrain.copy(),dftest.copy()\n",
    "    dftrain.index,dftest.index  = range(len(dftrain)),range(len(dftest))\n",
    "    dftrain.loc[0,['feature0','feature1','feature2']] = np.nan #构造若干缺失值\n",
    "    \n",
    "\n",
    "    # 训练xgboost模型\n",
    "    from tianjikit.runmodel import RunModel\n",
    "    model = RunModel(dftrain = dftrain,dftest = dftest,coverage_th=0, ks_th=0,\n",
    "            outliers_th=None, fillna_method= None, scale_method= None,selected_features=None)\n",
    "    xgb = model.train_xgb(cv=5, model_idx=5,\n",
    "          learning_rate=0.1,n_estimators=50, \n",
    "          max_depth=5, min_child_weight=30, \n",
    "          gamma=0,reg_alpha = 0,reg_lambda = 1,\n",
    "          subsample=0.8,colsample_bytree=0.8,\n",
    "          scale_pos_weight=1, n_jobs=4, seed=10) \n",
    "    model.test(xgb,dftest)\n",
    "    dfimportance = model.dfimportances['xgb']\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,dftrain,dftest = '',coverage_th = 0, ks_th = 0, outliers_th = None,\n",
    "                 fillna_method = 'infer',scale_method = 'MinMax', selected_features = None):\n",
    "        \n",
    "        # 校验是否有label列\n",
    "        assert 'label' in dftrain.columns, 'illegal input,there should be a  \"label\" column in dftrain!'\n",
    "        \n",
    "        # 校验label列的合法性\n",
    "        assert set(dftrain['label']) == {0,1},'illegal label values,label can only be 0 or 1!'\n",
    "        \n",
    "        self.dftrain = dftrain\n",
    "        self.dftest = dftest\n",
    "        \n",
    "        # 记录预处理参数信息\n",
    "        self.coverage_th = coverage_th\n",
    "        self.ks_th = ks_th\n",
    "        self.outliers_th = outliers_th\n",
    "        self.fillna_method = fillna_method\n",
    "        self.scale_method = scale_method\n",
    "        self.selected_features = selected_features\n",
    "        \n",
    "        X_train,y_train,X_test,y_test = self.preprocess_data(self.dftrain,self.dftest)\n",
    "        \n",
    "       \n",
    "        # 预处理后的训练和验证集\n",
    "        self.X_train,self.y_train = X_train,y_train\n",
    "        self.X_test,self.y_test  = X_test,y_test\n",
    "        \n",
    "        \n",
    "        # 特征重要性\n",
    "        self.dfimportances = {} \n",
    "        \n",
    "        # 报告信息\n",
    "        self.report_info = ''\n",
    "        \n",
    "        \n",
    "    def preprocess_data(self,dftrain,dftest):\n",
    "        \n",
    "        # 输出预处理提示信息\n",
    "        nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print('\\n================================================================================ %s\\n'%nowtime)\n",
    "        print('start data preprocessing ...\\n')\n",
    "        print('train set size:  {}'.format(len(dftrain)))\n",
    "        print('test set size:  {}'.format(len(dftest)))\n",
    "        print('coverage threshold:  {}'.format(self.coverage_th))\n",
    "        print('outlier threshold:  {}'.format(self.outliers_th))\n",
    "        print('ks threshold:  {}'.format(self.ks_th))\n",
    "        print('fillna method:  {}'.format(self.fillna_method))\n",
    "        print('scale method:  {}'.format(self.scale_method))\n",
    "        \n",
    "       \n",
    "        # 去掉['phone','id','idcard','id_card','loan_dt','name','id_map']等非特征列\n",
    "        for  col in {'phone','id','unique_id','uniq_id','idcard','id-card','id_card','name','loan_dt','idmap','id_map','id-map'}:\n",
    "            if col in dftrain.columns:\n",
    "                dftrain = dftrain.drop(col,axis = 1)\n",
    "                if len(dftest):dftest = dftest.drop(col,axis = 1)\n",
    "                    \n",
    "        # 校验是否存在非数值列 \n",
    "        try:\n",
    "            assert not np.dtype('O') in dftrain.dtypes.values\n",
    "        except:\n",
    "            object_cols = dftrain.columns[dftrain.dtypes == np.object].tolist()\n",
    "            print('removed feature columns not numerical: %s'%(','.join(map(str,object_cols))),file = sys.stderr)\n",
    "            dftrain = dftrain.drop(object_cols,axis = 1)\n",
    "            if len(dftest):dftest = dftest.drop(object_cols,axis = 1)\n",
    "        \n",
    "        # 如果selected_features 不为空，则进行特征筛选\n",
    "        if self.selected_features:\n",
    "            remained_cols = [col for col in dftrain.columns if col in self.selected_features + ['label']]\n",
    "            dftrain = dftrain[remained_cols]\n",
    "            if len(dftest):dftest = dftest[remained_cols]\n",
    "                \n",
    "        dftrain.columns = [self.__inverse_feature_dict.get(x,x) for x in dftrain.columns]\n",
    "        if len(dftest):dftest.columns = [self.__inverse_feature_dict.get(x,x) for x in dftest.columns]\n",
    "                    \n",
    "        # 分割feature和label\n",
    "        X_train = dftrain.drop(['label'],axis = 1)\n",
    "        y_train = dftrain[['label']]\n",
    "        X_test = dftest.drop(['label'],axis = 1) if len(dftest) else ''\n",
    "        y_test = dftest[['label']] if len(dftest) else ''\n",
    "        \n",
    "        print('original feature number:  {}'.format(X_train.shape[1]))\n",
    "        \n",
    "        # drop_outliers()\n",
    "        if self.outliers_th:\n",
    "            for col in X_train.columns:\n",
    "                X_train[col] = outliers.drop_outliers(X_train[col].values, X_train[col].values, alpha = self.outliers_th) \n",
    "                if len(dftest): X_test[col] = outliers.drop_outliers(X_train[col].values,X_test[col].values, alpha = self.outliers_th)  \n",
    "                    \n",
    "        # drop_feature()\n",
    "        X_train, X_test = dropfeature.drop_feature(X_train,y_train,X_test,coverage_threshold = self.coverage_th, \n",
    "                          ks_threshold = self.ks_th) \n",
    "        \n",
    "        print('feature number remain after dropfeature:  {}'.format(X_train.shape[1]))\n",
    "        \n",
    "        \n",
    "        # fill_nan()\n",
    "        if self.fillna_method:\n",
    "            X_train, X_test = fillnan.fill_nan(X_train,y_train,X_test,method = self.fillna_method)\n",
    "        \n",
    "        print('feature number increased to after fill_na:  {}'.format(X_train.shape[1]))\n",
    "        nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print('\\n================================================================================ %s\\n'%nowtime)\n",
    "        \n",
    "        # scale_feature()\n",
    "        if self.scale_method:\n",
    "            X_train, X_test  = scalefeature.scale_feature(X_train,X_test,method = self.scale_method)\n",
    "        \n",
    "        return(X_train,y_train,X_test,y_test)\n",
    "    \n",
    "    def train_xgb(self, cv = 5, model_idx = 1,    \n",
    "        learning_rate=0.1,n_estimators= 50,\n",
    "        max_depth=5, min_child_weight= 20,\n",
    "        gamma=0,reg_alpha = 0,reg_lambda = 1,\n",
    "        subsample=0.8,colsample_bytree=1, \n",
    "        n_jobs=-1, scale_pos_weight=1, seed=10,**kv):\n",
    "        \n",
    "        try:\n",
    "            xgb = XGBClassifier(learning_rate = learning_rate, n_estimators = n_estimators,\n",
    "                      max_depth = max_depth,min_child_weight = min_child_weight,\n",
    "                      gamma = gamma,reg_alpha = reg_alpha,reg_lambda = reg_lambda,\n",
    "                      subsample = subsample,colsample_bytree = colsample_bytree,\n",
    "                      n_jobs = n_jobs, scale_pos_weight = scale_pos_weight, seed = seed,**kv)\n",
    "        except:\n",
    "            xgb = XGBClassifier(learning_rate = learning_rate, n_estimators = n_estimators,\n",
    "                      max_depth = max_depth,min_child_weight = min_child_weight,\n",
    "                      gamma = gamma,reg_alpha = reg_alpha,reg_lambda = reg_lambda,\n",
    "                      subsample = subsample,colsample_bytree = colsample_bytree,\n",
    "                      nthread = n_jobs, scale_pos_weight = scale_pos_weight, seed = seed,**kv)\n",
    "            \n",
    "        info = \"start train xgboost model ...\"\n",
    "        print(info)\n",
    "        self.report_info = self.report_info + info + '\\n'\n",
    "        \n",
    "        clf = self.train(xgb,cv = cv,model_idx = model_idx) \n",
    "        \n",
    "        # 计算特征重要性\n",
    "        cols = self.X_train.columns\n",
    "        dfimportance = pd.DataFrame(clf.feature_importances_.reshape(-1),columns = ['importance'])\n",
    "        dfimportance.insert(0,'feature',cols)\n",
    "        try:\n",
    "            dfimportance = dfimportance.sort_values('importance',ascending= False)\n",
    "        except AttributeError as err:\n",
    "            dfimportance = dfimportance.sort('importance',ascending= False) \n",
    "        dfimportance['feature'] = [self.__feature_dict.get(x,x) for x in dfimportance['feature']]\n",
    "        self.dfimportances['xgb'] = dfimportance\n",
    "        return clf\n",
    "\n",
    "    \n",
    "    def train(self,clf,cv = 5,model_idx = 5):\n",
    "        \n",
    "        if cv:\n",
    "            #skf = StratifiedKFold(n_splits = cv,shuffle=True)\n",
    "\n",
    "            k,ks_mean_train,auc_mean_train,ks_mean_validate,auc_mean_validate = 0,0,0,0,0\n",
    "\n",
    "            models = {}\n",
    "\n",
    "            #for train_index,validate_index in skf.split(self.X_train,np.ravel(self.y_train)):\n",
    "            for train_index,validate_index in stratified_kfold(self.X_train,np.ravel(self.y_train),nfolds = cv):\n",
    "\n",
    "                k = k + 1\n",
    "                nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                info = '\\n{}: k = {}'.format(nowtime,k)\n",
    "                print(info)\n",
    "                self.report_info = self.report_info + info + '\\n'\n",
    "\n",
    "                X_train_k,y_train_k = self.X_train.iloc[train_index,:],self.y_train.iloc[train_index,:]\n",
    "                X_validate_k,y_validate_k = self.X_train.iloc[validate_index,:],self.y_train.iloc[validate_index,:]\n",
    "                \n",
    "                clf.fit(X_train_k,np.ravel(y_train_k))\n",
    "                predict_train_k = clf.predict_proba(X_train_k)[:,-1]\n",
    "                predict_validate_k = clf.predict_proba(X_validate_k)[:,-1]\n",
    "\n",
    "                dfks_train = ks.ks_analysis(predict_train_k,y_train_k.values)\n",
    "                dfks_validate = ks.ks_analysis(predict_validate_k,y_validate_k.values)\n",
    "\n",
    "                ks_train,ks_validate = max(dfks_train['ks_value']),max(dfks_validate['ks_value'])\n",
    "\n",
    "                auc_validate = metrics.roc_auc_score(np.ravel(y_validate_k), predict_validate_k)\n",
    "                auc_train = metrics.roc_auc_score(np.ravel(y_train_k),predict_train_k)\n",
    "\n",
    "                ks_mean_train = ks_mean_train + ks_train\n",
    "                auc_mean_train = auc_mean_train + auc_train\n",
    "                ks_mean_validate = ks_mean_validate + ks_validate\n",
    "                auc_mean_validate = auc_mean_validate + auc_validate\n",
    "\n",
    "                info = '\\ntrain: ks = {} \\t auc = {} '.format(ks_train,auc_train)\n",
    "                prettyks = ks.print_ks(predict_train_k,y_train_k.values)\n",
    "                info = info + '\\n' + str(prettyks) + '\\n'\n",
    "                info = info + '\\nvalidate: ks = {} \\t auc = {}'.format(ks_validate,auc_validate) + '\\n'\n",
    "                prettyks = ks.print_ks(predict_validate_k,y_validate_k.values)\n",
    "                info = info + str(prettyks) + '\\n'\n",
    "                print(info)\n",
    "                self.report_info = self.report_info + info\n",
    "                \n",
    "\n",
    "                models[k] = clf\n",
    "\n",
    "            ks_mean_train = ks_mean_train/float(k)\n",
    "            auc_mean_train = auc_mean_train/float(k)\n",
    "            ks_mean_validate = ks_mean_validate/float(k)\n",
    "            auc_mean_validate = auc_mean_validate/float(k)\n",
    "            \n",
    "            nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            info = '\\n================================================================================ %s\\n'%nowtime\n",
    "            info = info + 'train : ks mean {:.5f} ; auc mean {:.5f}'.format(ks_mean_train, auc_mean_train) + '\\n'\n",
    "            info = info + 'validate : ks mean {:.5f} ; auc mean {:.5f}'.format(ks_mean_validate, auc_mean_validate) + '\\n'\n",
    "            print(info)\n",
    "            self.report_info = self.report_info + info\n",
    "\n",
    "            clf = models[model_idx]\n",
    "            \n",
    "        # 处理 cv = 0 或 cv = None时无需交叉验证逻辑\n",
    "        else:\n",
    "            \n",
    "            nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            info = '\\n================================================================================ %s\\n'%nowtime\n",
    "            print(info)\n",
    "            self.report_info = self.report_info + info\n",
    "            \n",
    "            clf.fit(self.X_train,np.ravel(self.y_train))\n",
    "            predict_train = clf.predict_proba(self.X_train)[:,-1]\n",
    "            dfks_train = ks.ks_analysis(predict_train,self.y_train.values)\n",
    "            ks_train = max(dfks_train['ks_value'])   \n",
    "            auc_train = metrics.roc_auc_score(np.ravel(self.y_train),predict_train)\n",
    "            \n",
    "            info = '\\ntrain: ks = {} \\t auc = {} '.format(ks_train,auc_train) + '\\n'\n",
    "            prettyks = ks.print_ks(predict_train,self.y_train.values)\n",
    "            info = info + str(prettyks) + '\\n'\n",
    "            print(info)\n",
    "            self.report_info = self.report_info + info\n",
    "            \n",
    "        return(clf)\n",
    "        \n",
    "    def test(self,clf,dftest = pd.DataFrame()):\n",
    "        \n",
    "        info = \"\\nstart test model ... \"\n",
    "        print(info)\n",
    "        self.report_info = self.report_info + info + '\\n'\n",
    "        \n",
    "        # 若传入新的dftest，则需要再次做数据预处理\n",
    "        if len(dftest)>0:\n",
    "            \n",
    "            print('preprocessing test data...\\n')\n",
    "            \n",
    "            # 禁止数据预处理期间打印输出\n",
    "            stdout = sys.stdout\n",
    "            sys.stdout = open(os.devnull, 'w')\n",
    "            \n",
    "            X_train,y_train,X_test,y_test = self.preprocess_data(self.dftrain,dftest)\n",
    "            \n",
    "            # 恢复打印输出\n",
    "            sys.stdout = stdout\n",
    "            \n",
    "            # 预处理后的训练和测试集\n",
    "            self.X_train,self.y_train = X_train,y_train\n",
    "            self.X_test,self.y_test  = X_test,y_test\n",
    "            \n",
    "        \n",
    "        y_test_hat = clf.predict_proba(self.X_test)[:,-1]\n",
    "        dfks_test = ks.ks_analysis(y_test_hat,np.ravel(self.y_test))\n",
    "        ks_test = max(dfks_test['ks_value'])\n",
    "        auc_test = metrics.roc_auc_score(np.ravel(self.y_test),y_test_hat)\n",
    "        \n",
    "        info = 'test: ks = {} \\t auc = {} '.format(ks_test,auc_test) + '\\n'\n",
    "        prettyks = ks.print_ks(y_test_hat,np.ravel(self.y_test))\n",
    "        info = info + str(prettyks) + '\\n'\n",
    "        print(info)\n",
    "        self.report_info = self.report_info + info + '\\n'\n",
    "    \n",
    "    \n",
    "                \n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
